{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning with Pytorch\n",
    "\n",
    "In this notebook, we will implement deep q-learning algorithm using Pytorch to solve Atari games from OpenAI Gym. More specifically, we will be using `PongNoFrameskip-v4` environment. You can find more information of the Deep Q-Learning in the original [paper](https://www.datascienceassn.org/sites/default/files/Human-level%20Control%20Through%20Deep%20Reinforcement%20Learning.pdf).\n",
    "\n",
    "Lets start by importing necessary modules and packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 5607,
     "status": "ok",
     "timestamp": 1609307622738,
     "user": {
      "displayName": "Phone Thiha",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjF38sPI9I-rGMzKBm8HGwG-Pm-nhcjkS0Wue7iNw=s64",
      "userId": "12261723335667553247"
     },
     "user_tz": -390
    },
    "id": "bJT7SBCbu_0_"
   },
   "outputs": [],
   "source": [
    "# import necessary modules\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import gym.spaces\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import collections\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "# pytorch related\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3227,
     "status": "ok",
     "timestamp": 1609307622750,
     "user": {
      "displayName": "Phone Thiha",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjF38sPI9I-rGMzKBm8HGwG-Pm-nhcjkS0Wue7iNw=s64",
      "userId": "12261723335667553247"
     },
     "user_tz": -390
    },
    "id": "Nsjtob5lD7w6",
    "outputId": "97323612-6da1-4c6f-d3dd-eea4cb95c6a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using different gym wrappers to wrap our *pong env* so that we no need to do a lot of hardwork as described in the DQN paper such as frame-skipping, frame-stacking, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 1712,
     "status": "ok",
     "timestamp": 1609307630055,
     "user": {
      "displayName": "Phone Thiha",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjF38sPI9I-rGMzKBm8HGwG-Pm-nhcjkS0Wue7iNw=s64",
      "userId": "12261723335667553247"
     },
     "user_tz": -390
    },
    "id": "fzsuHYPkECsN"
   },
   "outputs": [],
   "source": [
    "# Creating ENV wrappers\n",
    "\n",
    "# ref: https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/blob/master/Chapter06/lib/wrappers.py\n",
    "# ref: https://github.com/openai/baselines/blob/master/baselines/common/atari_wrappers.py\n",
    "\n",
    "class FireResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None):\n",
    "        \"\"\"For environments where the user need to press FIRE for the game to start.\"\"\"\n",
    "        super(FireResetEnv, self).__init__(env)\n",
    "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
    "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.env.step(action)\n",
    "\n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "        obs, _, done, _ = self.env.step(1)\n",
    "        if done:\n",
    "            self.env.reset()\n",
    "        obs, _, done, _ = self.env.step(2)\n",
    "        if done:\n",
    "            self.env.reset()\n",
    "        return obs\n",
    "\n",
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None, skip=4):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super(MaxAndSkipEnv, self).__init__(env)\n",
    "        # most recent raw observations (for max pooling across time steps)\n",
    "        self._obs_buffer = collections.deque(maxlen=2)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for _ in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            self._obs_buffer.append(obs)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
    "        return max_frame, total_reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Clear past frame buffer and init. to first obs. from inner env.\"\"\"\n",
    "        self._obs_buffer.clear()\n",
    "        obs = self.env.reset()\n",
    "        self._obs_buffer.append(obs)\n",
    "        return obs\n",
    "\n",
    "\n",
    "class ProcessFrame84(gym.ObservationWrapper):\n",
    "    def __init__(self, env=None):\n",
    "        super(ProcessFrame84, self).__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, obs):\n",
    "        return ProcessFrame84.process(obs)\n",
    "\n",
    "    @staticmethod\n",
    "    def process(frame):\n",
    "        if frame.size == 210 * 160 * 3:\n",
    "            img = np.reshape(frame, [210, 160, 3]).astype(np.float32)\n",
    "        elif frame.size == 250 * 160 * 3:\n",
    "            img = np.reshape(frame, [250, 160, 3]).astype(np.float32)\n",
    "        else:\n",
    "            assert False, \"Unknown resolution.\"\n",
    "        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
    "        resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n",
    "        x_t = resized_screen[18:102, :]\n",
    "        x_t = np.reshape(x_t, [84, 84, 1])\n",
    "        return x_t.astype(np.uint8)\n",
    "\n",
    "\n",
    "class ImageToPyTorch(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(ImageToPyTorch, self).__init__(env)\n",
    "        old_shape = self.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], old_shape[0], old_shape[1]),\n",
    "                                                dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.moveaxis(observation, 2, 0)\n",
    "\n",
    "\n",
    "class ScaledFloatFrame(gym.ObservationWrapper):\n",
    "    def observation(self, obs):\n",
    "        return np.array(obs).astype(np.float32) / 255.0\n",
    "\n",
    "\n",
    "class BufferWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env, n_steps, dtype=np.float32):\n",
    "        super(BufferWrapper, self).__init__(env)\n",
    "        self.dtype = dtype\n",
    "        old_space = env.observation_space\n",
    "        self.observation_space = gym.spaces.Box(old_space.low.repeat(n_steps, axis=0),\n",
    "                                                old_space.high.repeat(n_steps, axis=0), dtype=dtype)\n",
    "\n",
    "    def reset(self):\n",
    "        self.buffer = np.zeros_like(self.observation_space.low, dtype=self.dtype)\n",
    "        return self.observation(self.env.reset())\n",
    "\n",
    "    def observation(self, observation):\n",
    "        self.buffer[:-1] = self.buffer[1:]\n",
    "        self.buffer[-1] = observation\n",
    "        return self.buffer\n",
    "\n",
    "def make_env(env_name):\n",
    "    env = gym.make(env_name)\n",
    "    env = MaxAndSkipEnv(env)\n",
    "    env = FireResetEnv(env)\n",
    "    env = ProcessFrame84(env)\n",
    "    env = ImageToPyTorch(env)\n",
    "    env = BufferWrapper(env, 4)\n",
    "    return ScaledFloatFrame(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After initializing the wrapper classes and functions, environment creation is straight forward just by calling `make_env(\"PongNoFrameskip-v4\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 1286,
     "status": "ok",
     "timestamp": 1609308832931,
     "user": {
      "displayName": "Phone Thiha",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjF38sPI9I-rGMzKBm8HGwG-Pm-nhcjkS0Wue7iNw=s64",
      "userId": "12261723335667553247"
     },
     "user_tz": -390
    },
    "id": "eXTr9XZFENBA"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6e084322e182>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# create env\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PongNoFrameskip-v4\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'make_env' is not defined"
     ]
    }
   ],
   "source": [
    "# create env\n",
    "env = make_env(\"PongNoFrameskip-v4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The env's state is already stacked into 4 consecutive frames (4x84x84), thanks to our wrappers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 949,
     "status": "ok",
     "timestamp": 1609308393840,
     "user": {
      "displayName": "Phone Thiha",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjF38sPI9I-rGMzKBm8HGwG-Pm-nhcjkS0Wue7iNw=s64",
      "userId": "12261723335667553247"
     },
     "user_tz": -390
    },
    "id": "R3j01n4JEXUu",
    "outputId": "e2bbf2f9-5664-428d-a1ee-b8deaedef183"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(0.0, 1.0, (4, 84, 84), float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The env has 6 action spaces namely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1301,
     "status": "ok",
     "timestamp": 1609308407327,
     "user": {
      "displayName": "Phone Thiha",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjF38sPI9I-rGMzKBm8HGwG-Pm-nhcjkS0Wue7iNw=s64",
      "userId": "12261723335667553247"
     },
     "user_tz": -390
    },
    "id": "ch96FDrcEXXr",
    "outputId": "a707f9bc-7e0c-43ce-8dba-d76006be6b8f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets visualize the preprocessed state-space image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get one frame\n",
    "frame = env.reset()\n",
    "\n",
    "# (4x84x84) -> (84x84x4)\n",
    "transposed_frame = frame.transpose(1,2,0)\n",
    "\n",
    "plt.imshow(transposed_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on, lets watch our agent playing pong. To do this, we cannot render directly on colab, so that we need to install some libs. We'll do that running the following cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!apt update && apt install xvfb\n",
    "!pip install gym-notebook-wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gnwrapper\n",
    "\n",
    "env = gnwrapper.Monitor(make_env(\"PongNoFrameskip-v4\"),directory=\"pong-video-v1\")\n",
    "\n",
    "total_reward = 0\n",
    "state = env.reset()\n",
    "\n",
    "while True:\n",
    "\n",
    "    state, reward, done, _ = env.step(env.action_space.sample())\n",
    "    total_reward += reward\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(\"Total reward: %.2f\" % total_reward)\n",
    "\n",
    "env.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell creates the deep q-network model `DQN` using Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, in_channels=4, n_actions=14):\n",
    "        \"\"\"\n",
    "        Initialize Deep Q Network\n",
    "        Args:\n",
    "            in_channels (int): number of input channels\n",
    "            n_actions (int): number of outputs\n",
    "        \"\"\"\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc4 = nn.Linear(7 * 7 * 64, 512)\n",
    "        self.head = nn.Linear(512, n_actions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.fc4(x.view(x.size(0), -1)))\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell is a `ReplayBuffer` class, which is used for **Experience Replay** technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 974,
     "status": "ok",
     "timestamp": 1609307670419,
     "user": {
      "displayName": "Phone Thiha",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjF38sPI9I-rGMzKBm8HGwG-Pm-nhcjkS0Wue7iNw=s64",
      "userId": "12261723335667553247"
     },
     "user_tz": -390
    },
    "id": "9BcopHvfEXas"
   },
   "outputs": [],
   "source": [
    "# experience replay\n",
    "\n",
    "Experience = namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def append(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
    "        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), \\\n",
    "               np.array(dones, dtype=np.uint8), np.array(next_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c54QUAG2Em3x"
   },
   "outputs": [],
   "source": [
    "##################\n",
    "# Hyperparameters\n",
    "##################\n",
    "\n",
    "# minibatch size\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# learning rate of Adam\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "# discount factor\n",
    "GAMMA = 0.99\n",
    "\n",
    "# initial exploration\n",
    "EPS_START = 1.0\n",
    "\n",
    "# final exploration\n",
    "EPS_END = 0.02\n",
    "\n",
    "# final exploration frame\n",
    "EPS_DECAY = 100000\n",
    "\n",
    "# target network update frequency\n",
    "TARGET_UPDATE = 1000\n",
    "\n",
    "# replay memory size\n",
    "INITIAL_MEMORY = 10000\n",
    "REPLAY_MEMORY = 10000\n",
    "\n",
    "# create Q-network\n",
    "q_network = DQN(n_actions=env.action_space.n).to(device)\n",
    "    \n",
    "# create Target-network\n",
    "target_network = DQN(n_actions=env.action_space.n).to(device)\n",
    "target_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "# adam optimizer\n",
    "optimizer = optim.Adam(q_network.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# create memory object for experience replay\n",
    "replay_buffer = ReplayBuffer(REPLAY_MEMORY)\n",
    "\n",
    "total_rewards = []\n",
    "total_reward = 0.0\n",
    "frame_idx = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code cell below to train our agent for `total_frames` frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 599
    },
    "executionInfo": {
     "elapsed": 1536440,
     "status": "error",
     "timestamp": 1609307131207,
     "user": {
      "displayName": "Phone Thiha",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjF38sPI9I-rGMzKBm8HGwG-Pm-nhcjkS0Wue7iNw=s64",
      "userId": "12261723335667553247"
     },
     "user_tz": -390
    },
    "id": "gS5s0my2Em_1",
    "outputId": "589ce3de-6b9a-4888-e255-2a2e7383a8c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames: 8957, Episodes: 10, Average reward: -20.6, Epsilon: 0.91043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:62: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames: 17599, Episodes: 20, Average reward: -20.6, Epsilon: 0.82401\n",
      "Frames: 26856, Episodes: 30, Average reward: -20.533333333333335, Epsilon: 0.73144\n",
      "Frames: 36332, Episodes: 40, Average reward: -20.525, Epsilon: 0.63668\n",
      "Frames: 47117, Episodes: 50, Average reward: -20.22, Epsilon: 0.52883\n",
      "Frames: 58740, Episodes: 60, Average reward: -20.016666666666666, Epsilon: 0.41259999999999997\n",
      "Frames: 72623, Episodes: 70, Average reward: -19.757142857142856, Epsilon: 0.27376999999999996\n",
      "Frames: 88584, Episodes: 80, Average reward: -19.625, Epsilon: 0.11416000000000004\n",
      "Frames: 108088, Episodes: 90, Average reward: -19.2, Epsilon: 0.02\n",
      "Frames: 131296, Episodes: 100, Average reward: -18.73, Epsilon: 0.02\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-f93e5fa81ffe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mloss_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_action_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected_state_action_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mloss_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training Complete!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    117\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m                    )\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_frames = 1000000\n",
    "\n",
    "state = env.reset()\n",
    "\n",
    "while frame_idx < total_frames:\n",
    "    frame_idx += 1\n",
    "    \n",
    "    #########################################################\n",
    "    # Epsilon-Greedy Policy action selection\n",
    "    epsilon = max(EPS_END, EPS_START - frame_idx / EPS_DECAY)\n",
    "\n",
    "    if np.random.random() < epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        state_a = np.array([state], copy=False)\n",
    "        state_v = torch.tensor(state_a).to(device)\n",
    "        q_vals_v = q_network(state_v)\n",
    "        _, act_v = torch.max(q_vals_v, dim=1)\n",
    "        action = int(act_v.item())\n",
    "    #########################################################\n",
    "\n",
    "    # do step in the environment\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    total_reward += reward\n",
    "\n",
    "    # experience replay\n",
    "    exp = Experience(state, action, reward, done, next_state)\n",
    "    replay_buffer.append(exp)\n",
    "    \n",
    "    state = next_state\n",
    "    \n",
    "    if done:\n",
    "        total_rewards.append(total_reward)\n",
    "        state = env.reset()\n",
    "        total_reward = 0.0\n",
    "        avg_reward = np.mean(total_rewards[-100:])\n",
    "        \n",
    "        # print every 10 episodes\n",
    "        if len(total_rewards) % 10 == 0:\n",
    "            print(f\"Frames: {frame_idx}, Episodes: {len(total_rewards)}, Average reward: {avg_reward}, Epsilon: {epsilon}\")\n",
    "\n",
    "        if avg_reward > 19.5:\n",
    "            print(f\"Environment is solved in {frame_idx} frames!\")\n",
    "            break\n",
    "                  \n",
    "    if len(replay_buffer) < INITIAL_MEMORY:\n",
    "        continue\n",
    "                  \n",
    "    if frame_idx % TARGET_UPDATE == 0:\n",
    "        target_network.load_state_dict(q_network.state_dict())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    batch = replay_buffer.sample(BATCH_SIZE)\n",
    "    \n",
    "    states, actions, rewards, dones, next_states = batch\n",
    "\n",
    "    states_v = torch.tensor(states).to(device)\n",
    "    next_states_v = torch.tensor(next_states).to(device)\n",
    "    actions_v = torch.tensor(actions).to(device)\n",
    "    rewards_v = torch.tensor(rewards).to(device)\n",
    "    done_mask = torch.ByteTensor(dones).to(device)\n",
    "\n",
    "    state_action_values = q_network(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
    "    \n",
    "    next_state_values = target_network(next_states_v).max(1)[0]\n",
    "    next_state_values[done_mask] = 0.0\n",
    "    next_state_values = next_state_values.detach()\n",
    "\n",
    "    expected_state_action_values = next_state_values * GAMMA + rewards_v\n",
    "                  \n",
    "    loss_t = nn.MSELoss()(state_action_values, expected_state_action_values)\n",
    "    loss_t.backward()\n",
    "    optimizer.step()\n",
    "                  \n",
    "print(\"Training Complete!\")\n",
    "torch.save(q_network.state_dict(), f\"pong-{total_frames}-v1.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, its time to test and watch our smart agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1149,
     "status": "ok",
     "timestamp": 1609307842123,
     "user": {
      "displayName": "Phone Thiha",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjF38sPI9I-rGMzKBm8HGwG-Pm-nhcjkS0Wue7iNw=s64",
      "userId": "12261723335667553247"
     },
     "user_tz": -390
    },
    "id": "ZmbKgi33N1MR",
    "outputId": "69411213-be1c-4bae-daac-2f8e844ca4e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_file_path = \"pong-1000000-v1.pt\"\n",
    "\n",
    "q_network = DQN(n_actions=env.action_space.n).to(device)\n",
    "q_network.load_state_dict(torch.load(saved_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7806,
     "status": "ok",
     "timestamp": 1609307908830,
     "user": {
      "displayName": "Phone Thiha",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjF38sPI9I-rGMzKBm8HGwG-Pm-nhcjkS0Wue7iNw=s64",
      "userId": "12261723335667553247"
     },
     "user_tz": -390
    },
    "id": "H8DtqpUMEnG0",
    "outputId": "61567b3d-9308-437e-b375-cc090d6d6600",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: 4.00\n",
      "Action counts: Counter({5: 795, 4: 599, 2: 580, 3: 533, 1: 435, 0: 276})\n"
     ]
    }
   ],
   "source": [
    "env = gnwrapper.Monitor(make_env(\"PongNoFrameskip-v4\"),directory=\"pong-video-v2\")\n",
    "\n",
    "state = env.reset()\n",
    "\n",
    "total_reward = 0\n",
    "\n",
    "while True:\n",
    "\n",
    "    state_a = np.array([state], copy=False)\n",
    "    state_v = torch.tensor(state_a).to(device)\n",
    "    q_vals_v = q_network(state_v)\n",
    "    _, act_v = torch.max(q_vals_v, dim=1)\n",
    "    action = int(act_v.item())\n",
    "\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(\"Total reward: %.2f\" % total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- https://github.com/openai/baselines/blob/master/baselines/common/atari_wrappers.py\n",
    "- https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/blob/master/Chapter06/"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyN8tQmGnvUyBkZSyUjODwgz",
   "collapsed_sections": [],
   "name": "w4d3_DQN_notebook.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
